# Observability Deep Discovery Session Playbook
# Purpose: Conduct in-depth observability discovery with DevOps, SRE, and platform teams

id: "PB_OBS_001"
name: "Observability Deep Discovery Session"
version: "1.0"
status: "ACTIVE"
playbook_mode: "ANALYTICAL"
playbook_category: "discovery_investigation"

metadata:
  category: "technical"
  framework: "Observability Discovery"
  team_owner: "specialists"
  specialty: "observability"
  description: "Deep technical discovery for observability use cases with DevOps, SREs, and platform engineers"

# Domain Evaluation Checklist - drives discovery questions
evaluation_checklist:
  id: "CL_OBS_001"
  path: "playbooks/specialists/observability/checklists/observability_evaluation_checklist.yaml"
  usage: "Guide discovery questions to assess current state against evaluation criteria"
  discovery_mapping:
    logs: "log_management"
    metrics: "metrics"
    traces: "tracing"
    alerting: "alerting"
    infrastructure: "platform"

raci:
  responsible:
    role: "observability_specialist"
    agent: "observability_specialist_agent"
  accountable:
    role: "sa_lead"
    human_required: true
  consulted:
    - role: "solution_architects"
    - role: "account_executives"
  informed:
    - role: "management"

triggers:
  - event: "observability_discovery_requested"
  - event: "monitoring_consolidation"
  - event: "sre_initiative"

inputs:
  required:
    - name: "customer_id"
      type: "string"
    - name: "discovery_objectives"
      type: "list"
    - name: "customer_attendees"
      type: "list"
  optional:
    - name: "current_monitoring_tools"
    - name: "known_pain_points"
    - name: "infrastructure_overview"

# Observability-Specific Discovery Questions
discovery_framework:
  team_and_culture:
    questions:
      - "How is your team structured? (DevOps, SRE, Platform, Dev)"
      - "Who is responsible for monitoring and observability?"
      - "How do you handle on-call? What's the current burden?"
      - "What's your incident management process?"
      - "Do you have defined SLOs? How are they measured?"

  current_tooling:
    questions:
      - "What monitoring tools are you using today?"
      - "How many different tools does your team use daily?"
      - "What works well? What's painful?"
      - "What's your total spend on observability tools?"
      - "How much time is spent on tool maintenance?"

  metrics:
    questions:
      - "What infrastructure are you monitoring?"
      - "What custom application metrics do you collect?"
      - "How do you handle high-cardinality metrics?"
      - "What's your metrics retention requirement?"
      - "How do you correlate metrics across services?"

  logs:
    questions:
      - "What's your daily log volume?"
      - "Are logs structured or unstructured?"
      - "How do you search and analyze logs today?"
      - "What's your log retention requirement?"
      - "How do you correlate logs with traces and metrics?"

  traces:
    questions:
      - "Are you doing distributed tracing today?"
      - "What's your instrumentation approach? (auto vs. manual)"
      - "Are you using OpenTelemetry?"
      - "What's your trace sampling strategy?"
      - "How do you use traces for troubleshooting?"

  alerting:
    questions:
      - "How many alerts do you have configured?"
      - "What's your alert-to-incident ratio?"
      - "How much alert fatigue does your team experience?"
      - "How do you handle alert routing and escalation?"
      - "What percentage of alerts are actionable?"

  infrastructure:
    questions:
      - "Cloud vs. on-prem split?"
      - "Are you running Kubernetes? How many clusters?"
      - "Microservices or monolithic architecture?"
      - "How many services/applications?"
      - "Multi-region deployment?"

steps:
  - step_id: "pre_session_research"
    name: "Pre-Session Research"
    outputs: ["customer_observability_profile"]
    research_areas:
      - "Customer tech stack"
      - "Industry reliability expectations"
      - "Known incidents (if public)"
      - "Current tool landscape"

  - step_id: "conduct_discovery"
    name: "Conduct Observability Discovery"
    outputs: ["discovery_notes"]
    session_structure:
      - phase: "Team & Culture (10 min)"
        focus: "Structure, on-call, maturity"
      - phase: "Current Tooling (15 min)"
        focus: "Tools, pain points, costs"
      - phase: "Three Pillars (25 min)"
        focus: "Metrics, logs, traces"
      - phase: "Alerting & Incidents (15 min)"
        focus: "Alert quality, incident response"
      - phase: "Infrastructure (10 min)"
        focus: "Scale, architecture, deployment"

  - step_id: "analyze_findings"
    name: "Analyze Observability Findings"
    outputs: ["observability_assessment"]
    analysis_areas:
      - "Observability maturity assessment"
      - "Tool consolidation opportunity"
      - "SRE practice recommendations"
      - "Quick wins identification"
      - "Cost optimization opportunity"

  - step_id: "document_requirements"
    name: "Document Requirements"
    outputs: ["requirements_document"]

outputs:
  format: "markdown"
  storage_path: "{realm}/{node}/external-infohub/specialists/observability/discovery_{date}.md"
  sections:
    - "Executive Summary"
    - "Team & Culture Assessment"
    - "Current Tooling Analysis"
    - "Three Pillars Assessment"
    - "Alerting & Incident Analysis"
    - "Infrastructure Overview"
    - "Recommendations"
    - "Next Steps"
